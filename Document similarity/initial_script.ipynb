{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "import gensim\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "# testing tokenization\n",
    "# opening files and generating clean lists\n",
    "data1 = []\n",
    "data2 = []\n",
    "f1 = open(\"file1.txt\")\n",
    "for line in f1:\n",
    "    # data1.append(line.replace(\"\\n\",\"\"))\n",
    "    data1.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', line))\n",
    "# print(data1)\n",
    "f1.close()\n",
    "\n",
    "f2 = open(\"file2.txt\")\n",
    "for line in f2:\n",
    "    # data2.append(line.replace(\"\\n\",\"\"))\n",
    "    data2.append(re.sub(r\"[^a-zA-Z0-9]+\", ' ', line))\n",
    "# print(data2)\n",
    "f2.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[['csv', 0.49], ['import', 0.87]]\n[]\n[['file', 0.31], ['insert', 0.21], ['open', 0.35], ['queries', 0.5], ['sql', 0.5], ['w', 0.5]]\n[['csv', 0.27], ['open', 0.34], ['8', 0.39], ['af', 0.39], ['author', 0.23], ['encoding', 0.39], ['r', 0.39], ['utf', 0.39]]\n[['csv', 0.39], ['af', 0.57], ['dictreader', 0.57], ['reader', 0.44]]\n[]\n[['reader', 0.86], ['line', 0.51]]\n[['insert', 0.12], ['author', 0.26], ['line', 0.3], ['0', 0.24], ['1', 0.22], ['2', 0.17], ['email', 0.44], ['exists', 0.15], ['fname', 0.42], ['format', 0.17], ['lname', 0.42], ['n', 0.13], ['query', 0.08], ['select', 0.28]]\n[['insert', 0.15], ['author', 0.16], ['line', 0.38], ['0', 0.15], ['1', 0.13], ['2', 0.21], ['email', 0.55], ['fname', 0.34], ['format', 0.21], ['lname', 0.34], ['n', 0.16], ['query', 0.1], ['values', 0.34]]\n[['file', 0.58], ['query', 0.28], ['write', 0.76]]\n[]\n[['csv', 0.25], ['open', 0.31], ['8', 0.36], ['encoding', 0.36], ['r', 0.36], ['utf', 0.36], ['articles', 0.44], ['f', 0.36]]\n[['csv', 0.39], ['dictreader', 0.57], ['reader', 0.44], ['f', 0.57]]\n[]\n[['reader', 0.86], ['line', 0.51]]\n[['author', 0.51], ['line', 0.4], ['print', 0.76]]\n[['line', 0.1], ['replace', 0.95], ['title', 0.28]]\n[['line', 0.23], ['replace', 0.87], ['journal', 0.44]]\n[['query', 1.0]]\n[['insert', 0.12], ['0', 0.24], ['1', 0.11], ['exists', 0.15], ['n', 0.13], ['query', 0.08], ['select', 0.28], ['as', 0.19], ['from', 0.28], ['into', 0.15], ['limit', 0.22], ['magazine', 0.34], ['name', 0.33], ['not', 0.19], ['tmp', 0.17], ['where', 0.55]]\n[['insert', 0.08], ['0', 0.08], ['1', 0.22], ['2', 0.12], ['exists', 0.1], ['n', 0.09], ['query', 0.06], ['select', 0.28], ['as', 0.13], ['into', 0.1], ['limit', 0.15], ['magazine', 0.12], ['name', 0.07], ['not', 0.13], ['tmp', 0.12], ['id', 0.46], ['number', 0.3], ['vol', 0.45], ['volume', 0.26], ['year', 0.39]]\n[['insert', 0.03], ['0', 0.07], ['1', 0.06], ['exists', 0.04], ['n', 0.04], ['query', 0.02], ['select', 0.24], ['title', 0.08], ['into', 0.04], ['magazine', 0.1], ['name', 0.06], ['tmp', 0.05], ['id', 0.38], ['number', 0.37], ['vol', 0.37], ['volume', 0.11], ['year', 0.11], ['3', 0.12], ['4', 0.08], ['art', 0.19], ['article', 0.12], ['mag', 0.08], ['pageno', 0.08], ['v', 0.61]]\n[['line', 0.62], ['format', 0.26], ['query', 0.25], ['title', 0.21], ['journal', 0.29], ['volume', 0.29], ['year', 0.29], ['pages', 0.42]]\n[]\n[['author', 0.15], ['line', 0.12], ['replace', 0.91], ['new', 0.26], ['str', 0.26]]\n[['name', 0.24], ['new', 0.49], ['str', 0.49], ['list', 0.49], ['split', 0.49]]\n[]\n[['print', 0.81], ['title', 0.59]]\n[['name', 0.44], ['list', 0.9]]\n[['name', 0.44], ['split', 0.9]]\n[['1', 0.52], ['lname', 0.67], ['name', 0.52]]\n[['0', 0.33], ['1', 0.3], ['fname', 0.38], ['name', 0.3], ['join', 0.75]]\n[['insert', 0.15], ['author', 0.33], ['0', 0.3], ['1', 0.28], ['exists', 0.19], ['fname', 0.36], ['lname', 0.36], ['n', 0.16], ['query', 0.1], ['select', 0.36], ['as', 0.24], ['into', 0.19], ['not', 0.24], ['tmp', 0.22], ['id', 0.22]]\n[['insert', 0.05], ['author', 0.12], ['0', 0.05], ['1', 0.05], ['2', 0.08], ['fname', 0.06], ['lname', 0.06], ['n', 0.06], ['query', 0.04], ['select', 0.26], ['title', 0.06], ['into', 0.07], ['magazine', 0.08], ['name', 0.05], ['id', 0.55], ['3', 0.1], ['art', 0.31], ['article', 0.21], ['aid', 0.13], ['au', 0.63]]\n[['line', 0.25], ['fname', 0.35], ['format', 0.42], ['lname', 0.35], ['query', 0.41], ['title', 0.35], ['journal', 0.48]]\n[['query', 0.39], ['print', 0.92]]\n[['file', 0.58], ['query', 0.28], ['write', 0.76]]\n[]\n[['file', 0.53], ['close', 0.85]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# performing operations on data1\n",
    "# lowering the case and tokenizing the sentense to words | put tokenized words in list so as corpora could give them unique id's\n",
    "# gen_docs_initial = [[w.lower() for w in word_tokenize(sentence)] for sentence in data1]\n",
    "# # removing stopwords\n",
    "gen_docs = []\n",
    "for sentence in data1:\n",
    "    temp_doc = []\n",
    "    for w in word_tokenize(sentence):\n",
    "        if w not in stopwords.words('english'):\n",
    "            temp_doc.append(w.lower())\n",
    "    gen_docs.append(temp_doc)\n",
    "\n",
    "# print(gen_docs)\n",
    "#\n",
    "# making dictionary for gensim to work on tokenized words | giving unique id's to each word\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "# print(dictionary.token2id)\n",
    "\n",
    "\n",
    "#\n",
    "# creating a corpus of bag of words and passing tokenized words to it. It calculates the no. of occurence of each word and maps it to the word's unique id's\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "# print(corpus)\n",
    "\n",
    "# using tfidf approach to balance out the less occuring and more frequent occuring words\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "\n",
    "# d = {dictionary.get(id): value for doc in tf_idf[corpus] for id, value in doc}\n",
    "# # print(d)\n",
    "\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "\n",
    "# generating similarity indexes\n",
    "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "244.45000000000002\nTotal similarity:  100\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# doing same operations on second set of data\n",
    "\n",
    "average_sims = []\n",
    "for line in data2:\n",
    "    # query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    temp_doc = []\n",
    "    for w in word_tokenize(line):\n",
    "        if w not in stopwords.words('english'):\n",
    "            temp_doc.append(w.lower())\n",
    "    query_doc = temp_doc\n",
    "    \n",
    "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "    query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "    # # print(query_doc_tf_idf)\n",
    "    # print('Comparing Result:', sims[query_doc_tf_idf]) \n",
    "    #calculating average similarity with the first set of data\n",
    "    # adding all similarity probabilities\n",
    "    sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "    # print(\"sum_of_sims\", sum_of_sims)\n",
    "    \n",
    "    # divide this total with count of data to calculate average similarity\n",
    "    average = round(sum_of_sims/len(data1) *100,2)\n",
    "    # print(average)\n",
    "    average_sims.append(average)\n",
    "    \n",
    "if np.sum(average_sims) > 100:\n",
    "    print(np.sum(average_sims))\n",
    "    print(\"Total similarity: \", 100)\n",
    "else:\n",
    "    print(\"Total similarity: \", np.sum(average_sims))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}